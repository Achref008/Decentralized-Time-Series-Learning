# Experimental Results

The following results demonstrate the performance of the **Decentralized Cooperative Control Learning (DCCL)** system across a simulated network of control nodes.

## 1. Multi-Node Convergence (200 Communication Rounds)

![Training and Validation Loss]([https://github.com/Achref008/Decentralized-Cooperative-Control-Learning-DCCL-/blob/main/outputs/Local%20Training%20Loss%20per%20Node%20in%20Decentralized%20Learning.PNG)
*This figure tracks the learning progress of 5 independent nodes (Node 0 to Node 4) over 200 decentralized synchronization rounds.*

**Observations:**
*   **Steady Convergence:** All 5 nodes show a consistent downward trend in both training (dotted/noisier lines) and validation (solid/smoother lines) loss, proving that the model successfully learns from the 500-iteration control dataset.
*   **Learning Heterogeneity:** While all nodes converge, they do so at different rates. **Node 4 (Cyan)** exhibits the fastest convergence, reaching a stable state by round 50, whereas **Node 3 (Green)** represents a slower learning curve, likely due to more complex local system dynamics.
*   **Final Stability:** By round 200, the network reaches a consensus with validation losses plateauing at an impressive **~0.02**, indicating high prediction accuracy for the PID gains.


Note: **The Line Chart** proves the **System Stability**: It shows that your decentralized algorithm doesn't "diverge" or crash over long periods (200 rounds).
---

## 2. Final Validation MSE Comparison

![Energy Distribution](https://github.com/Achref008/DTSL-Decentralized-Time-Series-Learning/blob/main/Images/Energy%20distribution%20across%20nodes.png)
*This bar chart compares the final Mean Validation MSE (Mean Squared Error) across 4 key nodes in the network.*

**Observations:**
*   **Network Consistency:** Despite training on different local data slices, the decentralized aggregation ensures that performance remains uniform across the network. All nodes achieve a final MSE between **0.85 and 0.96**.
*   **Consensus Quality:** The narrow gap between the best-performing node (Node 1) and the highest MSE node (Node 0) demonstrates that the **Byzantine-tolerant aggregation** and **ZMQ weight broadcasting** effectively pull all nodes toward a high-quality global model.

---

##  Experimental Setup
*   **Dataset:** 500 iterations of live PID control telemetry ($S_t$, $IAE$).
*   **Model Architecture:** Deep Feed-Forward Neural Network with Dropout (0.5) and $L_2$ Regularization.
*   **Optimization:** Adam Optimizer with a learning rate of $0.0001$.
*   **Decentralization:** Weight exchange via ZMQ PUB/SUB and robust consensus aggregation.

Note: **The Bar Chart** proves the **Model Fairness**: It shows that no single node is left behind with bad performance; **the "Global Brain"** helps every node reach a similar level of accuracy.
---


